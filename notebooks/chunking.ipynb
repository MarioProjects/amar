{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maparla/anaconda3/envs/ama-dev/lib/python3.12/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from src.processing.readers import PDFReader\n",
    "from src.processing.chunking import SymbolChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_OCR = False\n",
    "pdf_reader = PDFReader(enable_ocr=ENABLE_OCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digital PDF example\n",
    "pdf_path = \"notebooks/examples/digital.pdf\"\n",
    "digital_info = pdf_reader.get_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First page of the digital PDF:\n",
      "\n",
      "Sigmoid Loss for Language Image Pre-Training\n",
      "Xiaohua Zhai⋆\n",
      "Basil Mustafa\n",
      "Alexander Kolesnikov\n",
      "Lucas Beyer⋆\n",
      "Google DeepMind, Z¨\n",
      "urich, Switzerland\n",
      "{xzhai, basilm, akolesnikov, lbeyer}@google.com\n",
      "Abstract\n",
      "We propose a simple pairwise Sigmoid loss\n",
      "for\n",
      "Language-Image Pre-training (SigLIP). Unlike standard\n",
      "contrastive learning with softmax normalization, the sig-\n",
      "moid loss operates solely on image-text pairs and does not\n",
      "require a global view of the pairwise similarities for nor-\n",
      "malization.\n",
      "The sigmoid loss simultaneously allows fur-\n",
      "ther scaling up the batch size, while also performing bet-\n",
      "ter at smaller batch sizes. Combined with Locked-image\n",
      "Tuning, with only four TPUv4 chips, we train a SigLiT\n",
      "model that achieves 84.5% ImageNet zero-shot accuracy\n",
      "in two days. The disentanglement of the batch size from\n",
      "the loss further allows us to study the impact of exam-\n",
      "ples vs pairs and negative to positive ratio.\n",
      "Finally, we\n",
      "push the batch size to the extreme, up to one million, and\n",
      "ﬁnd that the beneﬁts of growing batch size quickly dimin-\n",
      "ish, with a more reasonable batch size of 32 k being suf-\n",
      "ﬁcient.\n",
      "We release our models at https://github.\n",
      "com/google-research/big_vision and hope our\n",
      "research motivates further explorations in improving the\n",
      "quality and efﬁciency of language-image pre-training.\n",
      "1. Introduction\n",
      "Contrastive pre-training using weak supervision from\n",
      "image-text pairs found on the web is becoming the go-to\n",
      "method for obtaining generic computer vision backbones,\n",
      "slowly replacing pre-training on large labelled multi-class\n",
      "datasets.\n",
      "The high-level idea is to simultaneously learn\n",
      "an aligned representation space for images and texts using\n",
      "paired data. Seminal works CLIP [36] and ALIGN [23] es-\n",
      "tablished the viability of this approach at a large scale, and\n",
      "following their success, many large image-text datasets be-\n",
      "came available privately [59, 13, 21, 49] and publicly [40,\n",
      "6, 15, 7, 41].\n",
      "The standard recipe to pre-train such models leverages\n",
      "the image-text contrastive objective. It aligns the image and\n",
      "⋆equal contribution\n",
      "Table 1: SigLiT and SigLIP results. Sigmoid loss is mem-\n",
      "ory efﬁcient, allows larger batch sizes (BS) that unlocks\n",
      "language image pre-training with a small number of chips.\n",
      "SigLiT model with a frozen public\n",
      "B/8 checkpoint [42],\n",
      "trained on the LiT image-text dataset [59] using four TPU-\n",
      "v4 chips for one day, achieves 79.7% 0-shot accuracy on\n",
      "ImageNet.\n",
      "The same setup with a g/14 checkpoint [58]\n",
      "leads to 84.5% accuracy, trained for two days. With a pub-\n",
      "lic unlocked\n",
      "B/16 image checkpoint [42], trained on the\n",
      "WebLI dataset [13], SigLIP achieves 71.0% 0-shot accu-\n",
      "racy using 16 TPU-v4 chips for three days. The last two\n",
      "rows show results with randomly initialized models.\n",
      "Image\n",
      "Text\n",
      "BS\n",
      "#TPUv4 Days\n",
      "INet-0\n",
      "SigLiT\n",
      "B/8\n",
      "L∗\n",
      "32 k\n",
      "4\n",
      "1\n",
      "79.8\n",
      "SigLiT\n",
      "g/14\n",
      "L\n",
      "20 k\n",
      "4\n",
      "2\n",
      "84.5\n",
      "SigLIP\n",
      "B/16\n",
      "B\n",
      "16 k\n",
      "16\n",
      "3\n",
      "71.0\n",
      "SigLIP\n",
      "B/16\n",
      "B\n",
      "32 k\n",
      "32\n",
      "2\n",
      "72.1\n",
      "SigLIP\n",
      "B/16\n",
      "B\n",
      "32 k\n",
      "32\n",
      "5\n",
      "73.4\n",
      "∗We use a variant of the L model with 12 layers.\n",
      "text embeddings for matching (positive) image-text pairs\n",
      "while making sure that unrelated (negative) image-text pairs\n",
      "are dissimilar in the embedding space. This is achieved via a\n",
      "batch-level softmax-based contrastive loss, applied twice to\n",
      "normalize the pairwise similarity scores across all images,\n",
      "then all texts. A naive implementation of the softmax is\n",
      "numerically unstable; it is usually stabilized by subtracting\n",
      "the maximum input value before applying the softmax [18],\n",
      "which requires another pass over the full batch.\n",
      "In this paper, we propose a simpler alternative: the sig-\n",
      "moid loss. It does not require any operation across the full\n",
      "batch and hence greatly simpliﬁes the distributed loss im-\n",
      "plementation and boosts efﬁciency. Additionally, it con-\n",
      "ceptually decouples the batch size from the deﬁnition of\n",
      "the task. We compare the proposed sigmoid loss with the\n",
      "standard softmax loss across multiple setups.\n",
      "In partic-\n",
      "ular, we investigate sigmoid-based loss with two promi-\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"First page of the digital PDF:\\n\\n{digital_info[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = SymbolChunker(words_limit=1024, overlap=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 chuncks found in the digital PDF\n"
     ]
    }
   ],
   "source": [
    "chunks = chunker.get_chunks(digital_info)\n",
    "print(f\"{len(chunks)} chuncks found in the digital PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 chuncks found in the first page of the digital PDF\n"
     ]
    }
   ],
   "source": [
    "first_page_chunks = [chunk for chunk in chunks if chunk.location == \"Page 1\"]\n",
    "print(f\"{len(first_page_chunks)} chuncks found in the first page of the digital PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk in the first page of the digital PDF:\n",
      "\n",
      "Sigmoid Loss for Language Image Pre-Training\n",
      "Xiaohua Zhai⋆\n",
      "Basil Mustafa\n",
      "Alexander Kolesnikov\n",
      "Lucas Beyer⋆\n",
      "Google DeepMind, Z¨\n",
      "urich, Switzerland\n",
      "{xzhai, basilm, akolesnikov, lbeyer}@google.com\n",
      "Abstract\n",
      "We propose a simple pairwise Sigmoid loss\n",
      "for\n",
      "Language-Image Pre-training (SigLIP). Unlike standard\n",
      "contrastive learning with softmax normalization, the sig-\n",
      "moid loss operates solely on image-text pairs and does not\n",
      "require a global view of the pairwise similarities for nor-\n",
      "malization.\n",
      "The sigmoid loss simultaneously allows fur-\n",
      "ther scaling up the batch size, while also performing bet-\n",
      "ter at smaller batch sizes. Combined with Locked-image\n",
      "Tuning, with only four TPUv4 chips, we train a SigLiT\n",
      "model that achieves 84.5% ImageNet zero-shot accuracy\n"
     ]
    }
   ],
   "source": [
    "first_chunk = first_page_chunks[0]\n",
    "print(f\"First chunk in the first page of the digital PDF:\\n\\n{first_chunk.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second chunk in the first page of the digital PDF:\n",
      "\n",
      "atch size, while also performing bet-\n",
      "ter at smaller batch sizes. Combined with Locked-image\n",
      "Tuning, with only four TPUv4 chips, we train a SigLiT\n",
      "model that achieves 84.5% ImageNet zero-shot accuracy\n",
      "in two days. The disentanglement of the batch size from\n",
      "the loss further allows us to study the impact of exam-\n",
      "ples vs pairs and negative to positive ratio.\n",
      "Finally, we\n",
      "push the batch size to the extreme, up to one million, and\n",
      "ﬁnd that the beneﬁts of growing batch size quickly dimin-\n",
      "ish, with a more reasonable batch size of 32 k being suf-\n",
      "ﬁcient.\n",
      "We release our models at https://github.\n",
      "com/google-research/big_vision and hope our\n",
      "research motivates further explorations in improving the\n",
      "quality and efﬁciency of language-image pre-training.\n",
      "1. Introduction\n"
     ]
    }
   ],
   "source": [
    "second_chunk = first_page_chunks[1]\n",
    "print(f\"Second chunk in the first page of the digital PDF:\\n\\n{second_chunk.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ama-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
